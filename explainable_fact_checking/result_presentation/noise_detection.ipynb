{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T13:57:50.019310Z",
     "start_time": "2024-09-02T13:57:50.003010Z"
    }
   },
   "cell_type": "code",
   "source": "cd /homes/bussotti/XFC2/code",
   "id": "edf687e7a15c2a6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/homes/bussotti/XFC2/code\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T13:57:54.765797Z",
     "start_time": "2024-09-02T13:57:50.021125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from explainable_fact_checking.result_presentation.notebook_utility import *"
   ],
   "id": "5c88a565644b5727",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bussotti/.conda/envs/feverous2/lib/python3.10/site-packages/spacy/util.py:910: UserWarning:\n",
      "\n",
      "[W095] Model 'en_core_web_sm' (3.1.0) was trained with spaCy v3.1.0 and may not be 100% compatible with the current version (3.7.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bussotti/.conda/envs/feverous2/bin/python\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ROC curve noise",
   "id": "75bc9a2939e9a1ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T13:58:00.821074Z",
     "start_time": "2024-09-02T13:57:54.767517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = xfc.load_preprocess_explanations(experiment_code_list=[\n",
    "    'fbs_np_1.0',\n",
    "    'fbs_np_2.0',\n",
    "    'lla_np_1.0',\n",
    "    'lla_np_2.0',\n",
    "])"
   ],
   "id": "9ae14195a292e084",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T13:58:00.872647Z",
     "start_time": "2024-09-02T13:58:00.825593Z"
    }
   },
   "cell_type": "code",
   "source": "df['model_id'].unique()",
   "id": "ff2675597ed57320",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['LLAMA3_1', 'feverous_verdict_predictor'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T13:58:02.445710Z",
     "start_time": "2024-09-02T13:58:00.874321Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "47836702979cae10",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model_path'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m/home/bussotti/.conda/envs/feverous2/lib/python3.10/site-packages/pandas/core/indexes/base.py:3652\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3651\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3652\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3653\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m/home/bussotti/.conda/envs/feverous2/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/home/bussotti/.conda/envs/feverous2/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'model_path'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mxfc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_preprocess_explanations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexperiment_code_list\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlla_np_1.0\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlla_np_2.0\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39munique()\n",
      "File \u001B[0;32m~/XFC2/code/explainable_fact_checking/plot/explanations_loader.py:243\u001B[0m, in \u001B[0;36mload_preprocess_explanations\u001B[0;34m(experiment_code_list, only_claim_exp_list, save_name)\u001B[0m\n\u001B[1;32m    240\u001B[0m explanation_df \u001B[38;5;241m=\u001B[39m explanations_to_df(exp_object_list)\n\u001B[1;32m    241\u001B[0m id_cols \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdataset_file_name\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_path\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m--> 243\u001B[0m \u001B[43mexplanation_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel_path\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnan\u001B[39m\u001B[38;5;124m'\u001B[39m, np\u001B[38;5;241m.\u001B[39mNaN, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    244\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m explanation_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_path\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39misna()\u001B[38;5;241m.\u001B[39many():\n\u001B[1;32m    245\u001B[0m     na_mask \u001B[38;5;241m=\u001B[39m explanation_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_path\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39misna()\n",
      "File \u001B[0;32m/home/bussotti/.conda/envs/feverous2/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3760\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3761\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3762\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3763\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m/home/bussotti/.conda/envs/feverous2/lib/python3.10/site-packages/pandas/core/indexes/base.py:3654\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3652\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3653\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m-> 3654\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3655\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3656\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3657\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3658\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3659\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'model_path'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T13:59:44.184886Z",
     "start_time": "2024-09-02T13:59:44.038326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "tdf = df.copy()\n",
    "tdf = tdf[tdf['type'] == 'evidence']"
   ],
   "id": "f5fa70f7f6b0ccf2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T13:59:44.336687Z",
     "start_time": "2024-09-02T13:59:44.285739Z"
    }
   },
   "cell_type": "code",
   "source": "tdf['model_id'].unique()",
   "id": "489f82a6dce15cef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['LLAMA3_1', 'feverous_verdict_predictor'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T13:59:53.202158Z",
     "start_time": "2024-09-02T13:59:53.164438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_best_f1(y_true, y_pred_proba):\n",
    "    precision, recall, f1_thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
    "    numerator = 2 * recall * precision\n",
    "    denom = recall + precision\n",
    "    f1_scores = np.divide(numerator, denom, out=np.zeros_like(denom), where=(denom != 0))\n",
    "    max_f1 = np.max(f1_scores)\n",
    "    max_f1_thresh = f1_thresholds[np.argmax(f1_scores)]\n",
    "    return max_f1_thresh, max_f1, f1_scores\n"
   ],
   "id": "9f281e909ae6f133",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T13:59:53.821961Z",
     "start_time": "2024-09-02T13:59:53.560058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate the auc, accuracy and best threshold for each model_id and explainer_name\n",
    "res_list = []\n",
    "index_cols = ['model_id', 'explainer_name',  #'predicted_label'\n",
    "              ]\n",
    "for keys, group_df in tdf.groupby(index_cols):\n",
    "    # model_id, explainer_name, predicted_label = keys\n",
    "    model_id, explainer_name = keys\n",
    "    tdict = dict(zip(index_cols, keys))\n",
    "    usefull_vs_noise_ground_truth = group_df['noisetag'] == 0\n",
    "    useful_score = group_df[['SUPPORTS', 'REFUTES']].abs().sum(axis=1)\n",
    "    fpr, tpr, thresholds = roc_curve(usefull_vs_noise_ground_truth, useful_score)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    best_threshold = thresholds[np.argmax(tpr - fpr)]\n",
    "    best_accuracy = accuracy_score(usefull_vs_noise_ground_truth, useful_score > best_threshold)\n",
    "\n",
    "    best_f1_th, best_f1_useful, f1_scores_useful = get_best_f1(usefull_vs_noise_ground_truth, useful_score)\n",
    "    best_f1_th_noise, best_f1_noise, f1_scores_noise = get_best_f1(~usefull_vs_noise_ground_truth, -useful_score)\n",
    "    # precision, recall, f1_thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
    "    tdict.update(auc=auc_score, best_threshold=best_threshold, best_accuracy=best_accuracy, fpr=fpr, tpr=tpr,\n",
    "                 # precision=precision, recall=recall,\n",
    "                 thresholds=thresholds, best_f1_useful=best_f1_useful, best_f1_th=best_f1_th, f1_scores_useful=f1_scores_useful,\n",
    "                 best_f1_noise=best_f1_noise, best_f1_th_noise=best_f1_th_noise, f1_scores_noise=f1_scores_noise\n",
    "                 )\n",
    "    res_list.append(tdict)\n",
    "roc_df = pd.DataFrame(res_list)"
   ],
   "id": "7d335e9c2335d185",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T13:59:54.347488Z",
     "start_time": "2024-09-02T13:59:54.258084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "avg_score = roc_df[['auc', 'best_accuracy']].mean().to_frame().T\n",
    "avg_score['model_id'] = 'average'\n",
    "avg_score['explainer_name'] = 'average'\n",
    "with_avg = pd.concat([roc_df,\n",
    "                      # avg_score\n",
    "                      ]).sort_values(by=index_cols)\n",
    "with_avg.drop(columns=['fpr', 'tpr', 'thresholds']).to_latex(os.path.join(save_path, 'roc_df.latex'), index=False,\n",
    "                                                             float_format='%.2f',\n",
    "                                                             bold_rows=True,\n",
    "                                                             caption='ROC curve results',\n",
    "                                                             label='tab:roc_df'\n",
    "                                                             )\n",
    "tcols = index_cols + ['best_f1_useful',\n",
    "                      'best_f1_th', 'best_f1_noise', 'best_f1_th_noise', 'auc', 'best_threshold', 'best_accuracy', ]\n",
    "roc_df[tcols].sort_values(by=index_cols).to_csv(os.path.join(save_path, 'roc_df.csv'))\n",
    "roc_df[tcols].drop(columns=['best_threshold', 'best_f1_th', 'best_f1_th_noise'])"
   ],
   "id": "652ff313f0c981e0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                     model_id explainer_name  best_f1_useful  best_f1_noise  \\\n",
       "0                    LLAMA3_1           lime        0.143006       0.970340   \n",
       "1                    LLAMA3_1           shap        0.133636       0.970340   \n",
       "2  feverous_verdict_predictor           lime        0.396832       0.971695   \n",
       "3  feverous_verdict_predictor           shap        0.402715       0.971780   \n",
       "\n",
       "        auc  best_accuracy  \n",
       "0  0.548724       0.792472  \n",
       "1  0.561789       0.734125  \n",
       "2  0.761034       0.848810  \n",
       "3  0.781050       0.805593  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>explainer_name</th>\n",
       "      <th>best_f1_useful</th>\n",
       "      <th>best_f1_noise</th>\n",
       "      <th>auc</th>\n",
       "      <th>best_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLAMA3_1</td>\n",
       "      <td>lime</td>\n",
       "      <td>0.143006</td>\n",
       "      <td>0.970340</td>\n",
       "      <td>0.548724</td>\n",
       "      <td>0.792472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LLAMA3_1</td>\n",
       "      <td>shap</td>\n",
       "      <td>0.133636</td>\n",
       "      <td>0.970340</td>\n",
       "      <td>0.561789</td>\n",
       "      <td>0.734125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feverous_verdict_predictor</td>\n",
       "      <td>lime</td>\n",
       "      <td>0.396832</td>\n",
       "      <td>0.971695</td>\n",
       "      <td>0.761034</td>\n",
       "      <td>0.848810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feverous_verdict_predictor</td>\n",
       "      <td>shap</td>\n",
       "      <td>0.402715</td>\n",
       "      <td>0.971780</td>\n",
       "      <td>0.781050</td>\n",
       "      <td>0.805593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "avg_score",
   "id": "87e5d36e860939d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The histogram of scores compared to true labels\n",
    "fig_hist = px.histogram(\n",
    "    x=useful_score, color=usefull_vs_noise_ground_truth, nbins=50,\n",
    "    labels=dict(color='True Labels', x='Score')\n",
    ")\n",
    "fig_hist.show()"
   ],
   "id": "519ef704f983e2e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = go.Figure()\n",
    "for i, row in roc_df.iterrows():\n",
    "    fig.add_trace(go.Scatter(x=row['fpr'], y=row['tpr'],\n",
    "                             mode='lines',\n",
    "                             name=f'{\" \".join([row[k] for k in index_cols])} auc={row[\"auc\"]:.2f}',\n",
    "                             line=dict(width=2),\n",
    "                             ))\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1],\n",
    "                         mode='lines',\n",
    "                         name='Random',\n",
    "                         line=dict(color='black', width=2, dash='dash')\n",
    "                         ))\n",
    "\n",
    "fig.update_xaxes(title='False Positive Rate')\n",
    "fig.update_yaxes(title='True Positive Rate')\n",
    "\n",
    "fig.update_layout(title='ROC curve',\n",
    "                  **layout_dict, **h_legend_dict)\n",
    "fig = end_fig_func(fig)\n",
    "fig.show()\n",
    "save_fig(fig, 'noise_detection_roc_curve')"
   ],
   "id": "513d344a7fe35014",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = go.Figure()\n",
    "for i, row in roc_df.iterrows():\n",
    "    fig.add_trace(go.Scatter(x=row['precision'], y=row['recall'],\n",
    "                             mode='lines',\n",
    "                             name=f'{\" \".join([row[k] for k in index_cols])} auc={row[\"auc\"]:.2f}',\n",
    "                             line=dict(width=2),\n",
    "                             ))\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1],\n",
    "                         mode='lines',\n",
    "                         name='Random',\n",
    "                         line=dict(color='black', width=2, dash='dash')\n",
    "                         ))\n",
    "\n",
    "fig.update_xaxes(title='False Positive Rate')\n",
    "fig.update_yaxes(title='True Positive Rate')\n",
    "\n",
    "fig.update_layout(title='ROC curve',\n",
    "                  **layout_dict, **h_legend_dict)\n",
    "fig = end_fig_func(fig)\n",
    "fig.show()"
   ],
   "id": "f90484b60ffc9294",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# F1 models on SUPPORTS and REFUTES",
   "id": "d694c4c696ea0f34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# take the first element for each id, model_id, dataset_file_name, for explainer_name 'LIME'\n",
    "# filter explainer_name 'LIME'\n",
    "LIME_mask = df['explainer_name'] == 'lime'\n",
    "# xclude dataset_file_name 'ex_AB_00.jsonl'\n",
    "normal_dataset_mask = df['dataset_file_name'] != 'ex_AB_00.jsonl'\n",
    "first_elements = df[LIME_mask & normal_dataset_mask].copy().groupby(['id', 'dataset_file_name', 'model_id'],\n",
    "                                                                    as_index=False).first()"
   ],
   "id": "639ac3d48680e62d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# define a function to compute the f1 score to be used in the groupby\n",
    "def f1_score_func(x):\n",
    "    predicted_dummies = pd.get_dummies(x['predicted_label'])\n",
    "    # if NEI is not present in the predicted_dummies add it with all zeros\n",
    "    if 'NEI' not in predicted_dummies.columns:\n",
    "        predicted_dummies['NEI'] = 0\n",
    "    true_dummies = pd.get_dummies(x['label'])\n",
    "    f1_score_list = []\n",
    "    for class_ in xfc.xfc_utils.class_names:\n",
    "        if class_ in true_dummies.columns:\n",
    "            f1_score_list.append(f1_score(true_dummies[class_], predicted_dummies[class_]))\n",
    "        else:\n",
    "            f1_score_list.append(np.nan)\n",
    "    return pd.Series(f1_score_list, index=xfc.xfc_utils.class_names)\n",
    "\n"
   ],
   "id": "e108cbb90ad8329a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# compute the f1 score for each id, model_id, dataset_file_name\n",
    "f1_score_df = first_elements.groupby(['model_id']).apply(f1_score_func)\n"
   ],
   "id": "f1bd31a9df0334b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "f1_score_df",
   "id": "d626d37b0de0cfb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "dcf8f9818927d28c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ",
   "id": "51c96ab2572f387e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
